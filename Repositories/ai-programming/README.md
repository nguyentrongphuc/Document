# Calculus

## Introduction

https://www.youtube.com/watch?v=ss3NSODhFJI

## Derivatives

https://www.youtube.com/watch?v=mzEit_Oe13E

## Derivatives Through Geometry

https://www.youtube.com/watch?v=mzllBDdtjIg

## The Chain Rule

https://www.youtube.com/watch?v=-Ti33VDPA7s

## Derivatives of exponentials

https://www.youtube.com/watch?v=QsEJGPVxsB8

## Implicit Differentiation

https://www.youtube.com/watch?v=2cNiF9ebdL0

## Limits

https://www.youtube.com/watch?v=5aL7Fn-9NnQ

## Integrals

https://www.youtube.com/watch?v=gKZvmxtM79Q

## More on Integrals

https://www.youtube.com/watch?v=OF3awXGM4k0

## The Taylor Series

- The Taylor Series is a representation of a function as an infinite sum of terms, or in other words, its a way to proximate a function.

- To build and train your own neural network you will not need to use the Taylor series. Since it is a main building block of approximating functions, a very useful tool in calculus, we decided not to skip this topic.


https://www.youtube.com/watch?v=1BLFuSXfgMY

## Multivariable Chain Rule

- In all of the videos above we concentrated on a function f(x)
- Notice, we had a single variable, x
- What happens when we have more than one variable in our equation?
- We often counter functions that are multivariable (more than one variable dependent). In Neural Networks that will almost always be the case. In this video we would like to focus on a very important and practical tool we saw previously, the chain rule. Only now, we will have more than a single variable.

https://www.youtube.com/watch?v=wXDy3P6F_P8

